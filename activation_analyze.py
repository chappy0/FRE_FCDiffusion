
import re
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from omegaconf import OmegaConf
from fcdiffusion.dataset import TestDataset
from ldm.util import instantiate_from_config
import numpy as np
import os

# å¯¼å…¥ä½ è‡ªå®šä¹‰çš„å·¥å…·å‡½æ•°ï¼ˆéœ€ç¡®ä¿ fcdiffusion_reduce_model.py ä¸­æœ‰å¯¹åº”å®ç°ï¼‰
from fcdiffusion_reduce_model import calculate_layer_influences, filter_layers_by_influence, read_text_from_file, parse_text
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from fcdiffusion.logger import ImageLogger
############################################
# 1. æ¨¡å‹åŠ è½½ä¸ç¯å¢ƒå‡†å¤‡
############################################
def load_model_from_config(config, ckpt, device=torch.device("cuda"), verbose=False):
    print(f"Loading model from {ckpt}")
    pl_sd = torch.load(ckpt, map_location="cpu")
    if "global_step" in pl_sd:
        print(f"Global Step: {pl_sd['global_step']}")
    sd = pl_sd["state_dict"]
    model = instantiate_from_config(config.model)
    missing, unexpected = model.load_state_dict(sd, strict=False)
    if len(missing) > 0 and verbose:
        print("missing keys:", missing)
    if len(unexpected) > 0 and verbose:
        print("unexpected keys:", unexpected)
    model.to(device)
    model.eval()
    return model, sd

def is_image_file(filename):
    IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.svg']
    return any(filename.lower().endswith(ext) for ext in IMAGE_EXTENSIONS)

def is_text_file(filename):
    return filename.lower().endswith('.txt')

def traverse_images_and_texts(directory):
    image_files = []
    text_contents = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if is_image_file(file):
                image_files.append(os.path.join(root, file))
            elif is_text_file(file):
                file_path = os.path.join(root, file)
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    text_contents.append(content)
    return image_files, text_contents

############################################
# 2. æ¿€æ´»å€¼é‡‡é›†
############################################
activation_values = {}
handles = []

def activation_hook(module, input, output, name):
    """è®°å½•æ¨¡å—çš„æ¿€æ´»å€¼å¹¶æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    key = f"{name}"
    # å¤„ç† outputï¼Œç¡®ä¿å…¶ä¸º NumPy æ•°ç»„
    if isinstance(output, tuple):  # å¦‚æœè¾“å‡ºæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
        output = output[0]
    if isinstance(output, list):  # å¦‚æœè¾“å‡ºæ˜¯åµŒå¥—åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå­åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
        output = output[0][0] if isinstance(output[0], list) else output[0]
    
    # å¦‚æœ output æ˜¯å¼ é‡ï¼Œè½¬æ¢ä¸º NumPy æ•°ç»„
    if isinstance(output, torch.Tensor):
        output = output.detach().cpu().numpy()
    
    # ç¡®ä¿ output æ˜¯ NumPy æ•°ç»„
    if not isinstance(output, np.ndarray):
        raise ValueError(f"Activation value for {key} is not a NumPy array: {type(output)}")

    # å­˜å‚¨æ¿€æ´»å€¼
    activation_values[key] = output

    # æ‰“å°æ¿€æ´»å€¼ç»Ÿè®¡ä¿¡æ¯
    # print(f"æ¿€æ´»å€¼ç»Ÿè®¡ï¼ˆ{key}ï¼‰ï¼šå½¢çŠ¶={output.shape}, å‡å€¼={np.mean(output):.4f}, æ–¹å·®={np.std(output):.4f}")
    with open("activation_stats_low.txt", "a",encoding='utf-8') as f:
        f.write(f"æ¿€æ´»å€¼ç»Ÿè®¡:{name},å½¢çŠ¶:{output.shape},å‡å€¼:{np.mean(output):.4f},æ–¹å·®:{np.std(output):.4f}\n")

def create_hook(name):
    return lambda module, input, output: activation_hook(module, input, output, name)

def register_hooks(model):
    # ä»…å¯¹ model.diffusion_model å†…çš„æ¨¡å—æ³¨å†Œé’©å­ï¼ˆä¸»è¦å…³æ³¨å·ç§¯å’Œæ®‹å·®å—ï¼‰
    if os.path.exists("activation_stats.txt"):
        os.remove("activation_stats.txt")
    # diffusion_model = model.model.diffusion_model
    
    for name, module in model.named_modules():
        # æ ¹æ®å®é™…æƒ…å†µï¼Œå¯ä»¥è¿‡æ»¤å‡ºå·ç§¯å±‚ã€ResBlock ç­‰
        # if "conv" in name.lower() or "resblock" in name.lower():
            hook_name = f"{name}_{type(module).__name__}"
            # hook_name = f"{name}"
            # print(f"hook_name:{hook_name}")
            handle = module.register_forward_hook(create_hook(hook_name))
            handles.append(handle)

############################################
# 3. æ„å»ºå‰ªæåçš„ UNet æ¨¡å—ï¼ˆä»…å‰ªæ diffusion_modelï¼‰
############################################


# def create_pruned_diffusion_model(original_diffusion_model, layers_to_keep):
#     """
#     æ ¹æ®æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯ç­›é€‰åï¼Œæ„é€ ä¸€ä¸ªæ–°çš„ UNetï¼ˆdiffusion_modelï¼‰ï¼Œ
#     ä»…ä¿ç•™åç§°åœ¨ layers_to_keep åˆ—è¡¨ä¸­çš„å±‚ã€‚
#     æ³¨æ„ï¼šlayers_to_keep ä¸­çš„åç§°ä¸º "model.diffusion_model.xxx" æ ¼å¼ï¼Œ
#     æ‰€ä»¥è¿™é‡Œéœ€è¦å‰¥ç¦»å‰ç¼€ä»¥åŒ¹é… diffusion_model å†…éƒ¨çš„å­æ¨¡å—åç§°ã€‚
#     """
#     class PrunedDiffusionModel(nn.Module):
#         def __init__(self, original_diffusion_model, layers_to_keep):
#             super().__init__()
#             self.pruned_layers = nn.ModuleList()
#             self.layers_to_keep = layers_to_keep
#             self.build_pruned_model(original_diffusion_model)

#         def build_pruned_model(self, original_module):
#             for name, module in original_module.named_children():
#                 full_name = f"model.diffusion_model.{name}_{type(module).__name__}"
#                 # æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦åœ¨å¾…ä¿ç•™åˆ—è¡¨ä¸­
#                 print(f"full_name:{full_name}")
#                 if any(layer.startswith(name) for layer in self.layers_to_keep):
#                     # å¦‚æœå½“å‰æ¨¡å—æ˜¯å¶å­èŠ‚ç‚¹ï¼ˆå³ç›´æ¥åŒ¹é…ï¼‰ï¼Œåˆ™æ·»åŠ åˆ°å‰ªææ¨¡å‹ä¸­

#                     if full_name in self.layers_to_keep:
#                         print("in keep")
#                         self.pruned_layers.append(module)
#                     else:
#                         # å¦‚æœå½“å‰æ¨¡å—ä¸æ˜¯å¶å­èŠ‚ç‚¹ï¼Œåˆ™é€’å½’å¤„ç†å…¶å­æ¨¡å—
#                         new_module = PrunedDiffusionModel(module, self.layers_to_keep)
#                         new_module.build_pruned_model(module)
#                         if len(new_module.pruned_layers) > 0:
#                             print("in keep children")
#                             self.pruned_layers.append(new_module)

#         def forward(self, x, c=None):
#             for layer in self.pruned_layers:
#                 x = layer(x)
#             return x

#     pruned_diffusion_model = PrunedDiffusionModel(original_diffusion_model, layers_to_keep)
#     return pruned_diffusion_model


# def create_pruned_diffusion_model(original_diffusion_model, layers_to_keep):
#     class PrunedDiffusionModel(nn.Module):
#         def __init__(self, parent_module, parent_path=""):
#             super().__init__()
#             self.layers = nn.ModuleDict()
#             self._build_pruned_structure(parent_module, parent_path)

#         def _build_pruned_structure(self, module, current_path):
#             """é€’å½’æ„å»ºä¿®å‰ªåçš„æ¨¡å‹ç»“æ„"""
#             for name, child_module in module.named_children():
#                 # æ„å»ºå®Œæ•´è·¯å¾„ï¼ˆå…¼å®¹ä¸¤ç§æ ¼å¼ï¼‰
#                 full_path = f"{current_path}.{name}" if current_path else name
#                 formatted_path_v1 = f"model.diffusion_model.{full_path}_{type(child_module).__name__}"
#                 # formatted_path_v2 = f"{full_path}_{type(child_module).__name__}"
#                 print(f"formatted_path_v1:{formatted_path_v1}")
#                 # æ£€æŸ¥æ˜¯å¦åœ¨ä¿ç•™åˆ—è¡¨ä¸­
#                 if any(layer in formatted_path_v1 for layer in layers_to_keep):
#                     print(f"child_module:{child_module}")
#                     self.layers[full_path] = child_module
#                     print(f"âœ… ä¿ç•™å±‚: {full_path} (åŒ¹é…åˆ° {formatted_path_v1} )")
#                 else:
#                     # é€’å½’å¤„ç†å­æ¨¡å—
#                     sub_module = PrunedDiffusionModel(child_module, full_path)
#                     if len(sub_module.layers) > 0:
#                         self.layers[full_path] = sub_module
#                         print(f"ğŸ” ä¿ç•™å­æ ‘: {full_path}")

#         def forward(self, x, c=None):
#             for name, layer in self.layers.items():
#                 if isinstance(layer, PrunedDiffusionModel):
#                     x = layer(x, c)
#                 else:
#                     x = layer(x)
#             return x

#     # åˆå§‹åŒ–å¹¶æ‰“å°æœ€ç»ˆç»“æ„
#     pruned_model = PrunedDiffusionModel(original_diffusion_model)
#     print("\næœ€ç»ˆå‰ªæç»“æ„ï¼š")
#     for name, module in pruned_model.layers.items():
#         print(f"â””â”€ {name} ({type(module).__name__})")
#     return pruned_model

# def create_pruned_diffusion_model(original_diffusion_model, layers_to_remove):
#     class PrunedDiffusionModel(nn.Module):
#         def __init__(self, parent_module, parent_path=""):
#             super().__init__()
#             self.layers = nn.ModuleDict()
#             self._build_pruned_structure(parent_module, parent_path.split("/"))

#         def _build_pruned_structure(self, module, path_parts):
#             """é€’å½’æ„å»ºä¿®å‰ªåçš„æ¨¡å‹ç»“æ„"""
#             for name, child_module in module.named_children():
#                 # æ„å»ºå½“å‰å±‚çº§è·¯å¾„
#                 current_path = "/".join(path_parts + [name])
#                 formatted_path = f"{current_path}_{type(child_module).__name__}"
#                 formatted_path = formatted_path[1::]
#                 print(f"formatted_path:{formatted_path}")
#                 # æ£€æŸ¥æ˜¯å¦åœ¨ä¿ç•™åˆ—è¡¨ä¸­
#                 if formatted_path in layers_to_keep:
#                     # é€å±‚åˆ›å»ºå­æ¨¡å—
#                     current = self
#                     for part in path_parts + [name]:
#                         if not hasattr(current, part):
#                             setattr(current, part, nn.Module())
#                         current = getattr(current, part)
#                     current.add_module(name, child_module)
#                     print(f"âœ… ä¿ç•™å±‚: {current_path}")
#                 else:
#                     # é€’å½’å¤„ç†å­æ¨¡å—
#                     sub_module = PrunedDiffusionModel(child_module, current_path)
#                     if len(sub_module.layers) > 0:
#                         self.layers[name] = sub_module

#         def forward(self, x, c=None):
#             for name, layer in self.layers.items():
#                 x = layer(x)
#             return x

#     return PrunedDiffusionModel(original_diffusion_model)
# def create_pruned_diffusion_model(original_diffusion_model, layers_to_keep):
#     class PrunedDiffusionModel(nn.Module):
#         def __init__(self):
#             super().__init__()
#             self.pruned_layers = nn.ModuleDict()
#             self.layers_to_keep = layers_to_keep
            
#             # é€’å½’éå†æ‰€æœ‰å­æ¨¡å—
#             for name, module in original_diffusion_model.named_modules():
#                 # ç§»é™¤å¯èƒ½çš„çˆ¶æ¨¡å—å‰ç¼€ï¼ˆå¦‚ "model.diffusion_model."ï¼‰
#                 clean_name = name.replace("model.diffusion_model.", "")
#                 if clean_name in self.layers_to_keep:
#                     # åŠ¨æ€æ„å»ºæ¨¡å—è·¯å¾„
#                     parts = clean_name.split('.')
#                     current = self
#                     for part in parts[:-1]:
#                         if not hasattr(current, part):
#                             setattr(current, part, nn.Module())
#                         current = getattr(current, part)
#                     # æ·»åŠ æ¨¡å—
#                     setattr(current, parts[-1], module)
#                     self.pruned_layers[clean_name] = module
            
#             print(f"å‰ªæåä¿ç•™çš„å±‚: {list(self.pruned_layers.keys())}")
#             if not self.pruned_layers:
#                 raise ValueError("å‰ªæåæ— æœ‰æ•ˆå±‚ï¼Œè¯·æ£€æŸ¥å±‚ç­›é€‰é˜ˆå€¼æˆ–åç§°æ ¼å¼ï¼")

#         def forward(self, x, c=None):
#             # æŒ‰åŸå§‹é¡ºåºæ‰§è¡Œä¿ç•™çš„å±‚
#             for name in self.layers_to_keep:
#                 module = self.pruned_layers.get(name)
#                 if module is not None:
#                     x = module(x)
#             return x
    
#     return PrunedDiffusionModel()

# def create_pruned_diffusion_model(original_diffusion_model, layers_to_remove):
#     class PrunedDiffusionModel(nn.Module):
#         def __init__(self, parent_module, parent_path=""):
#             super().__init__()
#             self.layers = nn.ModuleDict()
#             self._build_pruned_structure(parent_module, parent_path.split("."))

#         def _build_pruned_structure(self, module, path_parts):
#             """é€’å½’æ„å»ºä¿®å‰ªåçš„æ¨¡å‹ç»“æ„ï¼ˆç§»é™¤æ¨¡å¼ï¼‰"""
#             for name, child_module in module.named_children():
#                 # æ„å»ºå½“å‰æ¨¡å—çš„å®Œæ•´è·¯å¾„
#                 current_path = ".".join(path_parts + [name])
#                 formatted_path = f"model.diffusion_model{current_path}_{type(child_module).__name__}"
                
#                 # æ£€æŸ¥æ˜¯å¦åœ¨ç§»é™¤åˆ—è¡¨ä¸­
#                 if formatted_path in layers_to_remove:
#                     print(f"ğŸ—‘ï¸ ç§»é™¤å±‚: {formatted_path}")
#                     continue  # è·³è¿‡å½“å‰æ¨¡å—åŠå…¶æ‰€æœ‰å­æ¨¡å—
                
#                 # åˆ›å»ºå½“å‰å±‚çº§ç»“æ„
#                 current = self
#                 for part in path_parts + [name]:
#                     if not hasattr(current, part):
#                         setattr(current, part, nn.Module())
#                     current = getattr(current, part)
#                 print(f"current:{current}")
                
#                 # é€’å½’å¤„ç†å­æ¨¡å—
#                 if len(list(child_module.children())) > 0:
#                     sub_module = PrunedDiffusionModel(child_module, current_path)
#                     if len(sub_module.layers) > 0:
#                         current.add_module(name, sub_module)
#                 else:

#                     current.add_module(name, child_module)
#                     print(f"âœ… ä¿ç•™å±‚: {formatted_path,name,child_module}")

#         def forward(self, x, c=None):
#             print(f"layers:{self.layers.items()}")
#             for name, layer in self.layers.items():
#                 x = layer(x)
#             return x

#     return PrunedDiffusionModel(original_diffusion_model)


# def create_pruned_diffusion_model(original_diffusion_model, layers_to_remove):
#     class PrunedDiffusionModel(nn.Module):
#         def __init__(self, parent_module, parent_path=""):
#             super().__init__()
#             self.layers = nn.ModuleDict()
#             self._build_pruned_structure(parent_module, parent_path.split("."))

#         def _build_pruned_structure(self, module, path_parts):
#             """ä¼˜åŒ–åçš„é€’å½’æ„å»ºé€»è¾‘"""
#             for name, child_module in module.named_children():
#                 # æ„å»ºå®Œæ•´è·¯å¾„ï¼ˆæ— ç±»å‹åç¼€ï¼‰
#                 current_path = ".".join(path_parts + [name])
#                 formatted_path = f"model.diffusion_model{current_path}"
                
#                 # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»é™¤
#                 if formatted_path in layers_to_remove:
#                     print(f"ğŸ—‘ï¸ ç§»é™¤å±‚: {formatted_path}")
#                     continue
                
#                 # åˆ›å»ºå±‚çº§ç»“æ„
#                 current = self
#                 for part in path_parts + [name]:
#                     if not hasattr(current, part):
#                         # åŠ¨æ€æ·»åŠ æ¨¡å—å®¹å™¨
#                         container = nn.ModuleDict() if part.isdigit() else nn.Module()
#                         setattr(current, part, container)
#                     current = getattr(current, part)
                
#                 # æ·»åŠ å¶å­èŠ‚ç‚¹
#                 if len(list(child_module.children())) == 0:
#                     if isinstance(current, nn.ModuleDict):
#                         current[name] = child_module
#                     else:
#                         setattr(current, name, child_module)
#                     print(f"âœ… ä¿ç•™å±‚: {formatted_path}")
#                 else:
#                     # é€’å½’å¤„ç†å­æ¨¡å—
#                     sub_module = PrunedDiffusionModel(child_module, current_path)
#                     if len(sub_module.layers) > 0:
#                         if isinstance(current, nn.ModuleDict):
#                             current[name] = sub_module
#                         else:
#                             setattr(current, name, sub_module)

#         def forward(self, x, c=None):
#             for name, layer in self.layers.items():
#                 x = layer(x)
#             return x

#     return PrunedDiffusionModel(original_diffusion_model)

# åœ¨ create_pruned_diffusion_model ä¸­æ·»åŠ è°ƒè¯•ä¿¡æ¯å’Œè·¯å¾„åŒ¹é…ä¿®æ­£
def create_pruned_diffusion_model(original_diffusion_model, layers_to_remove):
    class PrunedDiffusionModel(nn.Module):
        def __init__(self, parent_module, parent_path=""):
            super().__init__()
            self.layers = nn.ModuleDict()
            self._build_pruned_structure(parent_module, parent_path.split("."))

        def _build_pruned_structure(self, module, path_parts):
            """ä¼˜åŒ–åçš„é€’å½’æ„å»ºé€»è¾‘"""
            for name, child_module in module.named_children():
                # æ„å»ºå®Œæ•´è·¯å¾„ï¼ˆæ— ç±»å‹åç¼€ï¼‰
                current_path = ".".join(path_parts + [name])
                formatted_path = f"model.diffusion_model{current_path}"
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»é™¤
                if formatted_path in layers_to_remove:
                    print(f"ğŸ—‘ï¸ ç§»é™¤å±‚: {formatted_path}")
                    continue
                
                # åˆ›å»ºå±‚çº§ç»“æ„
                current = self
                for part in path_parts + [name]:
                    if not hasattr(current, part):
                        # åŠ¨æ€æ·»åŠ æ¨¡å—å®¹å™¨
                        container = nn.ModuleDict() if part.isdigit() else nn.Module()
                        setattr(current, part, container)
                    current = getattr(current, part)
                
                # æ·»åŠ å¶å­èŠ‚ç‚¹
                if len(list(child_module.children())) == 0:
                    if isinstance(current, nn.ModuleDict):
                        current[name] = child_module
                    else:
                        setattr(current, name, child_module)
                    print(f"âœ… ä¿ç•™å±‚: {formatted_path}")
                else:
                    # é€’å½’å¤„ç†å­æ¨¡å—
                    sub_module = PrunedDiffusionModel(child_module, current_path)
                    if len(sub_module.layers) > 0:
                        if isinstance(current, nn.ModuleDict):
                            current[name] = sub_module
                        else:
                            setattr(current, name, sub_module)

        def forward(self, x, c=None):
            for name, layer in self.layers.items():
                x = layer(x)
            return x

    return PrunedDiffusionModel(original_diffusion_model)


# def remove_layers_from_model(model, layers_to_remove):
#     """
#     ç›´æ¥åœ¨æ¨¡å‹ä¸Šç§»é™¤æŒ‡å®šçš„å±‚
#     :param model: åŸå§‹æ¨¡å‹
#     :param layers_to_remove: éœ€è¦ç§»é™¤çš„å±‚è·¯å¾„åˆ—è¡¨ï¼ˆæ ¼å¼ä¸º "model.diffusion_model.xxx"ï¼‰
#     """
#     for name, module in model.named_modules():
#         # ç§»é™¤å‰ç¼€å¹¶å¤„ç†è·¯å¾„æ ¼å¼
#         # clean_name = name.replace("model.diffusion_model.", "")
#         # print(f'layers_to_remove:{layers_to_remove}')
#         clean_name = name
#         print(f"clean_name:{clean_name}")
#         if clean_name in layers_to_remove:
#             # è·å–çˆ¶æ¨¡å—å’Œå±‚åç§°
#             parent_name, layer_name = os.path.split(name)
#             parent_module = model.get_submodule(parent_name)
#             print(f"parent_module:{parent_module,layer_name}")
#             # æ£€æŸ¥çˆ¶æ¨¡å—æ˜¯å¦ä¸º ModuleDict æˆ– ModuleList
#             if isinstance(parent_module, nn.ModuleDict):
#                 print('del module')
#                 del parent_module[layer_name]
#             elif isinstance(parent_module, nn.ModuleList):
#                 # éœ€è¦å¤„ç†ç´¢å¼•è½¬æ¢ï¼Œæ¯”è¾ƒå¤æ‚
#                 print('need to handle')
#                 pass
#             else:
#                 print('del common module')
#                 # å¯¹äºæ™®é€šæ¨¡å—ï¼Œç›´æ¥åˆ é™¤å±æ€§
#                 delattr(parent_module, layer_name)
#             print(f"ğŸ—‘ï¸ å·²ç§»é™¤å±‚: {name}")


# def remove_layers_from_model(model, layers_to_remove):
#     """
#     ç›´æ¥åœ¨æ¨¡å‹ä¸Šç§»é™¤æŒ‡å®šçš„å±‚
#     :param model: åŸå§‹æ¨¡å‹
#     :param layers_to_remove: éœ€è¦ç§»é™¤çš„å±‚è·¯å¾„åˆ—è¡¨ï¼ˆæ ¼å¼ä¸º "model.diffusion_model.xxx"ï¼‰
#     """
#     for target_path in layers_to_remove:
#         # ç§»é™¤å‰ç¼€å¹¶å¤„ç†è·¯å¾„æ ¼å¼
#         clean_path = target_path.replace("model.diffusion_model.", "")
#         path_parts = clean_path.split('.')
        
#         # éå†åˆ°ç›®æ ‡æ¨¡å—çš„çˆ¶æ¨¡å—
#         current_module = model
#         for part in path_parts[:-1]:
#             if hasattr(current_module, part):
#                 current_module = getattr(current_module, part)
#             else:
#                 print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„è·¯å¾„: {'.'.join(path_parts[:i+1])}")
#                 break
        
#         # è·å–ç›®æ ‡å±‚åç§°
#         layer_name = path_parts[-1]
        
#         # æ£€æŸ¥ç›®æ ‡å±‚æ˜¯å¦å­˜åœ¨
#         if hasattr(current_module, layer_name):
#             # åˆ é™¤ç›®æ ‡å±‚
#             delattr(current_module, layer_name)
#             print(f"ğŸ—‘ï¸ å·²ç§»é™¤å±‚: {target_path}")
#         else:
#             print(f"âš ï¸ å±‚ä¸å­˜åœ¨ï¼Œè·³è¿‡: {target_path}")

def remove_layers_from_model(model, layers_to_remove):
    """
    ç§»é™¤æŒ‡å®šå±‚ï¼Œå¹¶é€’å½’æ¸…ç†ç©ºçˆ¶æ¨¡å—
    """
    removed_paths = set()  # è®°å½•å·²ç§»é™¤çš„è·¯å¾„

    for target_path in layers_to_remove:
        # å¤„ç†è·¯å¾„æ ¼å¼
        clean_path = target_path.replace("model.diffusion_model.", "")
        path_parts = clean_path.split('.')
        
        # éå†åˆ°ç›®æ ‡æ¨¡å—çš„çˆ¶æ¨¡å—
        parent_module = model
        for part in path_parts[:-1]:
            if hasattr(parent_module, part):
                parent_module = getattr(parent_module, part)
            else:
                break
        
        # å°è¯•åˆ é™¤ç›®æ ‡å±‚
        layer_name = path_parts[-1]
        if hasattr(parent_module, layer_name):
            delattr(parent_module, layer_name)
            removed_paths.add(target_path)
            print(f"ğŸ—‘ï¸ å·²ç§»é™¤å±‚: {target_path}")
            
            # é€’å½’æ£€æŸ¥çˆ¶æ¨¡å—æ˜¯å¦ä¸ºç©º
            current_module = parent_module
            for i in reversed(range(len(path_parts)-1)):
                module_part = path_parts[i]
                ancestor = model
                for p in path_parts[:i+1]:
                    ancestor = getattr(ancestor, p)
                
                # å¦‚æœå½“å‰æ¨¡å—æ²¡æœ‰å­æ¨¡å—ï¼Œåˆ™åˆ é™¤
                if len(list(ancestor.children())) == 0:
                    grandparent = model
                    for p in path_parts[:i]:
                        grandparent = getattr(grandparent, p)
                    delattr(grandparent, module_part)
                    removed_paths.add(".".join(path_parts[:i+1]))
                    print(f"ğŸ—‘ï¸ ç§»é™¤ç©ºçˆ¶æ¨¡å—: {'.'.join(path_parts[:i+1])}")

    # äºŒæ¬¡éªŒè¯ç¡®ä¿ç§»é™¤æˆåŠŸ
    for path in removed_paths:
        clean_path = path.replace("model.diffusion_model.", "")
        try:
            model.get_submodule(clean_path)
            print(f"âš ï¸ è­¦å‘Š: å±‚æœªè¢«æ­£ç¡®ç§»é™¤: {path}")
        except AttributeError:
            continue

############################################
# 5. æƒé‡åŠ è½½ï¼šä»…åŠ è½½å‰ªæ diffusion_model çš„æƒé‡
############################################
# def load_pruned_diffusion_weights(pruned_diffusion_model, original_state_dict, original_diffusion_model):
#     new_state_dict = {}
#     for name, module in pruned_diffusion_model.named_modules():
#         try:
#             orig_module = original_diffusion_model.get_submodule(name)
#         except Exception:
#             continue
#         for param_name, param in module.named_parameters(recurse=False):
#             orig_key = f"{name}.{param_name}"
#             if orig_key in original_state_dict:
#                 new_state_dict[orig_key] = original_state_dict[orig_key]
#     pruned_diffusion_model.load_state_dict(new_state_dict, strict=False)
#     return pruned_diffusion_model
def load_pruned_diffusion_weights(pruned_model, original_state_dict):
    # æ„å»ºè·¯å¾„æ˜ å°„è¡¨
    path_mapping = {}
    for new_path, _ in pruned_model.named_modules():
        orig_path = new_path.replace("_", ".")  # å¤„ç†æ•°å­—ç´¢å¼•æ¨¡å—
        path_mapping[new_path] = f"model.diffusion_model.{orig_path}"
    
    # åŠ è½½æƒé‡
    new_state_dict = {}
    for new_path, param in pruned_model.named_parameters():
        orig_path = path_mapping.get(new_path, new_path)
        if orig_path in original_state_dict:
            new_state_dict[new_path] = original_state_dict[orig_path]
    
    pruned_model.load_state_dict(new_state_dict, strict=False)
    return pruned_model


def load_weights_after_pruning(model, original_state_dict):
    """
    å‰ªæååŠ è½½æƒé‡
    """
    model.load_state_dict(original_state_dict, strict=False)
    print("æƒé‡åŠ è½½å®Œæˆï¼ˆéä¸¥æ ¼æ¨¡å¼ï¼‰")
    return model
############################################
# # 6. å¾®è°ƒè®­ç»ƒï¼ˆæ•´ä¸ªæ¨¡å‹ï¼‰
# ############################################

class PrunedFCDiffusionModel(pl.LightningModule):  # æ”¹ä¸ºç»§æ‰¿LightningModule
    def __init__(self, original_model, pruned_diffusion_model, learning_rate=1e-5, sd_locked=True):
        super().__init__()
        # ä¿ç•™åŸå§‹æ¨¡å‹å¼•ç”¨
        self.original_model = original_model
        
        # æ›¿æ¢diffusion_model
        self.diffusion_model = pruned_diffusion_model
        self.control_model = original_model.control_model
        self.cond_stage_model = original_model.cond_stage_model
        self.first_stage_model = original_model.first_stage_model
        
        # ç»§æ‰¿å¿…è¦å±æ€§å’Œå‚æ•°
        self.learning_rate = learning_rate
        self.sd_locked = sd_locked
        self.first_stage_key = original_model.first_stage_key
        self.control_mode = original_model.control_mode
        self.ucg_training = original_model.ucg_training if hasattr(original_model, 'ucg_training') else {}
        self.use_scheduler = original_model.use_scheduler if hasattr(original_model, 'use_scheduler') else False

         # ç¡®ä¿å‰ªææ¨¡å‹çš„å‚æ•°å¯è®­ç»ƒ
        for param in self.diffusion_model.parameters():
            param.requires_grad = True  # å¼ºåˆ¶è§£å†»å‰ªæéƒ¨åˆ†

    # def forward(self, x, c):
    #     return self.diffusion_model(x, c)
    # def forward(self, x, c, *args, **kwargs):
    #     print(f"self.shorten_cond_schedule:{self.shorten_cond_schedule}")
        
    #     t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()
    #     if self.model.conditioning_key is not None:
    #         assert c is not None
    #         if self.cond_stage_trainable:
    #             c = self.get_learned_conditioning(c)
    #         if self.shorten_cond_schedule:  # TODO: drop this option
    #             tc = self.cond_ids[t].to(self.device)
    #             c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))
    #     return self.p_losses(x, c, t, *args, **kwargs)


    def forward(self, x, c, *args, **kwargs):
        # ç”Ÿæˆéšæœºæ—¶é—´æ­¥ (ä¸åŸå§‹å®ç°ä¸€è‡´)
        t = torch.randint(
            0, self.original_model.num_timesteps, 
            (x.shape[0],), 
            device=self.device
        ).long()

        # å¤„ç†æ¡ä»¶ä¿¡æ¯
        if self.original_model.cond_stage_trainable:
            c = self.original_model.get_learned_conditioning(c)

        # è°ƒç”¨æ ¸å¿ƒæŸå¤±è®¡ç®—
        return self.original_model.p_losses(x, c, t, *args, **kwargs)
    @torch.no_grad()
    def get_input(self, batch, k, *args, **kwargs):
        print(f"k:{k}")
        return self.original_model.get_input(batch, k, *args, **kwargs)

    def shared_step(self, batch):
        x, c = self.get_input(batch, self.first_stage_key)
        loss = self(x, c)
        return loss, {"loss": loss}


    def configure_optimizers(self):
        # æ”¶é›†æ‰€æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°
        trainable_params = []
        # æ‰©æ•£æ¨¡å‹å‚æ•°ï¼ˆå‰ªæåï¼‰
        trainable_params += list(self.diffusion_model.parameters())
        # å…¶ä»–æ¨¡å—å‚æ•°ï¼ˆæ ¹æ® sd_locked å†³å®šï¼‰
        if not self.sd_locked:
            trainable_params += list(self.control_model.parameters())
            trainable_params += list(self.cond_stage_model.parameters())
            trainable_params += list(self.first_stage_model.parameters())
        
        optimizer = torch.optim.AdamW(trainable_params, lr=self.learning_rate)
        return optimizer

    def training_step(self, batch, batch_idx):
        # å‰å‘ä¼ æ’­ï¼ˆç¡®ä¿è¾“å‡ºä¿ç•™æ¢¯åº¦ï¼‰
        # print(f'self.first_stage_key:{self.first_stage_key}')
        # x, c = self.get_input(batch, self.first_stage_key)
        # output = self(x, c)
        loss, loss_dict = self.shared_step(batch)
        # è®¡ç®—æŸå¤±ï¼ˆå‡è®¾æ¨¡å‹ç›´æ¥è¿”å›æŸå¤±å€¼ï¼‰
        # loss = output
        self.log("train_loss", loss, prog_bar=True)
        return loss


# åœ¨å‰ªæå‰æ·»åŠ æ ¼å¼å¤„ç†
def clean_layer_names(layers):
    cleaned = []
    for layer in layers:
        # ç§»é™¤ç±»å‹åç¼€å’Œå¤šä½™å‰ç¼€
        clean = layer.replace("model.diffusion_model.", "")
        clean = re.sub(r"_\w+$", "", clean)  # ç§»é™¤_Conv2dç­‰åç¼€
        clean = 'diffusion_model.' + clean
        cleaned.append(clean)
    return cleaned


############################################
# 7. ä¸»æµç¨‹
############################################
if __name__ == "__main__":
    # åŠ è½½é…ç½®ä¸æ£€æŸ¥ç‚¹
    yaml_file_path = "configs/model_config.yaml"
    # ckpt_file_path = r'D:\paper\FCDiffusion_code-main\lightning_logs\fcdiffusion_mid_pass_checkpoint\epoch=11-step=241999.ckpt'
    ckpt_file_path = r"D:\paper\FRE_FCD\lightning_logs\low\epoch=5-step=17999_SA.ckpt"
    config = OmegaConf.load(yaml_file_path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model, sd = load_model_from_config(config, ckpt_file_path, device)
    
    # æ³¨å†Œé’©å­ï¼Œä»…åœ¨ diffusion_model å†…é‡‡é›†æ¿€æ´»æ•°æ®
    register_hooks(model)
    
    # éå†æµ‹è¯•æ•°æ®ï¼ˆå›¾åƒä¸æ–‡æœ¬ï¼‰
    directory_path = r'D:\paper\FCDiffusion_code-main\datasets\test'
    image_files, text_contents = traverse_images_and_texts(directory_path)
    test_res_num = 1
    dataset = TestDataset(image_files[0], text_contents[0], test_res_num)
    dataloader = DataLoader(dataset, num_workers=0, batch_size=1, shuffle=False)
    
    # å‰å‘ä¼ æ’­é‡‡é›†æ¿€æ´»å€¼activation_stats
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            # å‡è®¾ model.get_input ç”¨äºæ•°æ®é¢„å¤„ç†ï¼ˆä¾‹å¦‚ "jpg" å›¾åƒï¼‰
            x, c = model.get_input(batch, 'jpg')
            _ = model(x, c)
    
    # ç§»é™¤æ‰€æœ‰é’©å­ï¼Œé˜²æ­¢åç»­å¹²æ‰°
    for handle in handles:
        handle.remove()
    
    # è§£ææ¿€æ´»ç»Ÿè®¡æ–‡ä»¶ï¼Œè®¡ç®—æ¯ä¸€å±‚çš„å½±å“åŠ›ï¼Œå¹¶ç­›é€‰å‡ºéœ€è¦ä¿ç•™çš„å±‚
    text = read_text_from_file("activation_stats_low.txt")
    layers = parse_text(text)
    layers_influences = calculate_layer_influences(layers, alpha=2.0, beta=1.0)
    # è¿™é‡Œè®¾ç½®çš„é˜ˆå€¼å¯æ ¹æ®å®é™…æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯è°ƒæ•´
    influence_threshold = 1.3
    layers_to_keep,prune_layers = filter_layers_by_influence(layers_influences, threshold=influence_threshold)
    # print(f"æ ¹æ®æ¿€æ´»å€¼ç­›é€‰åï¼Œåœ¨ diffusion_model ä¸­ä¿ç•™çš„å±‚: {layers_to_keep}")
    
    # # åœ¨è°ƒç”¨ create_pruned_diffusion_model å‰æ·»åŠ ä»¥ä¸‹ä»£ç 
    # cleaned_layers = []
    # for layer in prune_layers:
    #     # ç§»é™¤å‰ç¼€å¹¶æ›¿æ¢åˆ†éš”ç¬¦
    #     clean_layer = layer.replace("model.diffusion_model.", "").replace(".", "/")
    #     # clean_layer = layer.replace(".", "/")
    #     # æ·»åŠ ç±»å‹åç¼€
    #     module_type = layer.split("_")[-1]
    #     cleaned_layers.append(f"{clean_layer}")
    # layers_to_remove = cleaned_layers
    # æ„é€ å‰ªæåçš„ diffusion_modelï¼ˆUNet æ¨¡å—ï¼‰
    
    # prune_layers = clean_layer_names(prune_layers)
    # # print(f"layers_to_remove:{prune_layers}")
    # # pruned_diffusion_model = create_pruned_diffusion_model(model.model.diffusion_model, prune_layers)
    # print(f"å‰ªæå‰:{model.model.diffusion_model}")
    # remove_layers_from_model(model.model, prune_layers)
    # print(f"å‰ªæå:{model.model.diffusion_model}")
    # # print(f"pruned_diffusion_model:{pruned_diffusion_model}")
    # # pruned_diffusion_model = load_pruned_diffusion_weights(pruned_diffusion_model, c, model.model.diffusion_model)
    # pruned_diffusion_model = load_weights_after_pruning(model.model.diffusion_model,sd)
    # # ç»„è£…åŒ…å«å‰ªæ diffusion_model çš„å®Œæ•´ FCDiffusion æ¨¡å‹ï¼Œå…¶å®ƒæ¨¡å—ä¿æŒåŸæ ·
    # pruned_full_model = PrunedFCDiffusionModel(model, pruned_diffusion_model).to(device)
    

    # pruned_lightning_model = PrunedFCDiffusionModel(
    #     original_model=model,
    #     pruned_diffusion_model=pruned_diffusion_model,
    #     learning_rate=1e-5,
    #     sd_locked=True
    # ).to(device)

    # # é…ç½®è®­ç»ƒå‚æ•°ï¼ˆä¸åŸè®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
    # batch_size = 4
    # logger_freq = 500
    # val_every_n_train_steps = 1000
    # max_epochs = 100
    #     # å›è°ƒé…ç½®
    # logger = ImageLogger(root_path='pruned_logs', batch_frequency=logger_freq)
    # checkpoint_callback = ModelCheckpoint(
    #     dirpath='pruned_checkpoints',
    #     every_n_train_steps=val_every_n_train_steps,
    #     save_top_k=-1
    # )
    
    # # é…ç½®Trainer
    # trainer = pl.Trainer(
    #     gpus=1,
    #     precision=32,
    #     max_epochs=max_epochs,
    #     callbacks=[logger, checkpoint_callback],
    #     # enable_progress_bar=True
    # )
    
    # # å¼€å§‹è®­ç»ƒ
    # trainer.fit(pruned_lightning_model, dataloader)
    # # å¾®è°ƒè®­ç»ƒ
    # # trained_pruned_model = fine_tune_pruned_model(pruned_full_model, dataloader, device, epochs=20)
    
    # # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆåŒ…æ‹¬é…ç½®ä¿¡æ¯ä¸å‰ªæå±‚è®°å½•ï¼‰
    # torch.save({
    #     'state_dict': pruned_lightning_model.state_dict(),
    #     'config': config,
    #     'pruned_layers': layers_to_keep
    # }, 'pruned_final_model.ckpt')
    
    # print("å‰ªæå¹¶å¾®è°ƒåçš„æ¨¡å‹å·²ä¿å­˜åˆ° pruned_final_model.ckpt")
