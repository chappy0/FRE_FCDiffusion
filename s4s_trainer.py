# s4s_trainer.py

from omegaconf import OmegaConf
import torch
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import os
import logging
from tqdm import tqdm

# å‡è®¾æ‚¨çš„æ•™å¸ˆæ¨¡å‹ã€U-Netç­‰éƒ½å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯¼å…¥
from ldm.util import instantiate_from_config
from stages_step_optim import NoiseScheduleVP 

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# --- é…ç½® ---
CONFIG_PATH = "configs/stable-diffusion/v2-inference.yaml"
CKPT_PATH = r"D:\paper\FCDiffusion_code-main\models\v2-1_512-ema-pruned.ckpt"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- è¾“å…¥ï¼šé˜¶æ®µä¸€å’Œé˜¶æ®µäºŒçš„äº§å‡º ---
# ç”±30æ­¥æœ€ä¼˜æ•™å¸ˆç”Ÿæˆçš„â€œé»„é‡‘â€æ½œå˜é‡è½¨è¿¹æ•°æ®é›†
TEACHER_TRAJECTORY_DATA_PATH = "teacher_golden_dataset/latents.npy" 
STUDENT_SCHEDULE_PATH = "msos_optimized_schedules/student_nfe4_schedule.txt"

# --- è¾“å‡ºï¼šé˜¶æ®µä¸‰çš„äº§å‡º ---
OUTPUT_DIR = "s4s_learned_coeffs"
NFE = 4
ORDER = 4 # S4Sæ±‚è§£å™¨çš„é˜¶æ•°ï¼Œå†³å®šäº†coeffsçš„å½¢çŠ¶

# --- è®­ç»ƒå‚æ•° ---
LEARNING_RATE = 1e-3
EPOCHS = 50
BATCH_SIZE = 256

def get_s4s_update(x_curr, eps_t, history_eps, coeffs_step):
    """ è®¡ç®—å•æ­¥S4Sæ›´æ–° """
    correction = torch.zeros_like(eps_t)
    num_history_to_use = min(len(history_eps), len(coeffs_step))
    for i in range(num_history_to_use):
        correction += coeffs_step[i] * (eps_t - history_eps[i])
    d_t = eps_t + correction
    return d_t

def s4s_solver_trajectory(x_T, ts, model, coeffs):
    """ ä½¿ç”¨ç»™å®šçš„S4Sç³»æ•°ï¼Œå®Œæ•´åœ°èµ°ä¸€æ¡NFEæ­¥çš„è½¨è¿¹ """
    x_curr = x_T.clone()
    eps_history = []
    
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡å’Œè¯„ä¼°æ¨¡å¼
    model.to(DEVICE).eval()

    for i in range(len(ts) - 1):
        t_curr, t_next = ts[i], ts[i+1]
        
        # é¢„æµ‹eps
        time_cond = torch.full((x_curr.shape[0],), t_curr * 999, device=DEVICE, dtype=torch.long)
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†CFGï¼Œå®é™…è®­ç»ƒæ—¶åº”ä¼ å…¥conditioning
        # ä¸ºäº†ä¸“æ³¨äºç³»æ•°å­¦ä¹ ï¼Œæˆ‘ä»¬å‡è®¾è¾“å…¥æ˜¯æ— æ¡ä»¶çš„
        with torch.no_grad():
            eps_pred = model.apply_model(x_curr, time_cond, None) # ç®€åŒ–ä¸ºæ— æ¡ä»¶

        # è®¡ç®—S4Sæ–¹å‘
        d_t = get_s4s_update(eps_pred, eps_history, coeffs[i])
        
        # DDIMæ›´æ–°
        alpha_t = model.alphas_cumprod[int(t_curr*999)].sqrt()
        alpha_next = model.alphas_cumprod[int(t_next*999)].sqrt()
        sigma_t = (1 - alpha_t**2).sqrt()
        sigma_next = (1 - alpha_next**2).sqrt()
        
        x_curr = (alpha_next / alpha_t) * x_curr + (sigma_next - alpha_next * sigma_t / alpha_t) * d_t

        eps_history.insert(0, eps_pred.detach())
        if len(eps_history) > ORDER - 1:
            eps_history.pop()
            
    return x_curr


def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 1. åŠ è½½U-Netæ¨¡å‹ (åªéœ€è¦å®ƒçš„apply_model)
    logging.info("Loading U-Net model...")
    config = OmegaConf.load(CONFIG_PATH)
    model = instantiate_from_config(config.model)
    ckpt = torch.load(CKPT_PATH, map_location="cpu")
    model.load_state_dict(ckpt.get("state_dict", ckpt), strict=False)
    model.to(DEVICE)

    # 2. åŠ è½½æ•™å¸ˆæ•°æ®é›†å’Œå­¦ç”Ÿè°ƒåº¦
    logging.info(f"Loading teacher dataset from {TEACHER_TRAJECTORY_DATA_PATH}")
    teacher_data = np.load(TEACHER_TRAJECTORY_DATA_PATH)
    # å‡è®¾æ•°æ®æ ¼å¼: [num_samples, latent_channels, H, W]
    # æˆ‘ä»¬éœ€è¦åˆå§‹å™ªå£°x_Tå’Œæ•™å¸ˆæœ€ç»ˆç”Ÿæˆçš„x_0
    x_T_teacher = torch.randn_like(torch.from_numpy(teacher_data[:1])) # å‡è®¾æ‰€æœ‰æ ·æœ¬ä»ç›¸åŒçš„é«˜æ–¯å™ªå£°å¼€å§‹
    x_0_teacher = torch.from_numpy(teacher_data).to(DEVICE).float()
    
    dataset = TensorDataset(x_0_teacher)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    logging.info(f"Loading student schedule from {STUDENT_SCHEDULE_PATH}")
    student_ts = torch.from_numpy(np.loadtxt(STUDENT_SCHEDULE_PATH, dtype=np.float32)).to(DEVICE)

    # 3. åˆå§‹åŒ–å¯å­¦ä¹ çš„S4Sç³»æ•°
    # å½¢çŠ¶: [NFE, Order-1]
    s4s_coeffs = torch.randn(NFE, ORDER - 1, device=DEVICE, requires_grad=True)

    # 4. è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW([s4s_coeffs], lr=LEARNING_RATE)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

    logging.info("ğŸš€ Starting S4S coefficient training...")
    for epoch in range(EPOCHS):
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        total_loss = 0
        for batch_x0_teacher, in pbar:
            batch_x_T = x_T_teacher.repeat(batch_x0_teacher.shape[0], 1, 1, 1)
            
            optimizer.zero_grad()
            
            # ä½¿ç”¨å½“å‰ç³»æ•°ï¼Œè®¡ç®—å­¦ç”Ÿçš„æœ€ç»ˆè¾“å‡º
            x_0_student = s4s_solver_trajectory(batch_x_T, student_ts, model, s4s_coeffs)
            
            # è®¡ç®—ä¸æ•™å¸ˆè¾“å‡ºçš„MSEæŸå¤±
            loss = F.mse_loss(x_0_student, batch_x0_teacher)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.6f}"})
        
        scheduler.step()
        logging.info(f"Epoch {epoch+1} average loss: {total_loss / len(dataloader):.6f}")

    # 5. ä¿å­˜å­¦åˆ°çš„ç³»æ•°
    final_coeffs_np = s4s_coeffs.detach().cpu().numpy()
    output_path = os.path.join(OUTPUT_DIR, f"student_nfe{NFE}_coeffs.npy")
    np.save(output_path, final_coeffs_np)
    
    logging.info(f"\nğŸ‰ğŸ‰ğŸ‰ S4S training complete! Coefficients saved to: {output_path}")

if __name__ == '__main__':
    # æ³¨æ„ï¼šè¿è¡Œæ­¤è„šæœ¬å‰ï¼Œæ‚¨éœ€è¦å…ˆç”Ÿæˆæ•™å¸ˆçš„é»„é‡‘æ•°æ®é›†
    # ä¾‹å¦‚ï¼šç”¨ä¸€ä¸ªè„šæœ¬åŠ è½½teacher_nfe30_schedule.txtï¼Œç”Ÿæˆä¸€æ‰¹å›¾åƒï¼Œå¹¶ä¿å­˜å®ƒä»¬çš„æ½œå˜é‡x_0
    main()