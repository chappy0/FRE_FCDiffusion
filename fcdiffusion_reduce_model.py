# #fcdiffusion_reduce_model.py
# import re
# from collections import namedtuple

# # å®šä¹‰ä¸€ä¸ªæ•°æ®ç»“æ„æ¥å­˜å‚¨æ¯ä¸€å±‚çš„ä¿¡æ¯
# LayerStats = namedtuple("LayerStats", ["name", "shape", "mean", "variance"])

# # ä»æ–‡ä»¶ä¸­è¯»å–æ–‡æœ¬
# def read_text_from_file(file_path):
#     with open(file_path, "r", encoding="utf-8") as file:
#         text = file.read()
#     return text

# # è§£ææ–‡æœ¬
# def parse_text(text):
#     layers = []
#     # æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œå…è®¸å¤šä½™çš„ç©ºæ ¼å’Œä¸åŒçš„æ‹¬å·
#     pattern = re.compile(r"æ¿€æ´»å€¼ç»Ÿè®¡\s*[\(ï¼ˆ](.*?)[\)ï¼‰]\s*ï¼š\s*å½¢çŠ¶\s*=\s*[\(ï¼ˆ](.*?)[\)ï¼‰]\s*,\s*å‡å€¼\s*=\s*([-+]?\d*\.\d+|\d+)\s*,\s*æ–¹å·®\s*=\s*([-+]?\d*\.\d+|\d+)\s*")
#     matches = pattern.findall(text)

#     if not matches:
#         print("æ­£åˆ™è¡¨è¾¾å¼æœªåŒ¹é…åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼")
#         return layers

#     for match in matches:
#         name, shape, mean, variance = match
#         layers.append(LayerStats(name, shape, float(mean), float(variance)))

#     return layers

# # # è®¡ç®—å½±å“åŠ›
# def calculate_layer_influences(layers, alpha=2.0, beta=1.0):
#     influences = []
#     for layer in layers:
#         influence = alpha * abs(layer.mean) + beta * layer.variance
#         influences.append((layer.name, influence))
#     return influences

# # def calculate_layer_influences(model, alpha=1.0, beta=1.0):
# #     layers = model.layers  # å‡è®¾æ¨¡å‹æœ‰ä¸€ä¸ªlayerså±æ€§ï¼Œé‡Œé¢åŒ…å«äº†æ‰€æœ‰çš„å±‚
# #     influences = []
# #     for layer in layers:
# #         mean = layer.mean  # å‡è®¾æ¯ä¸ªå±‚æœ‰meanå’Œvarianceå±æ€§
# #         variance = layer.variance
# #         influence = alpha * abs(mean) + beta * variance
# #         influences.append((layer, influence))
# #     return influences


# def filter_layers_by_influence(layers_influences, threshold=0.1):
#     # ç­›é€‰å½±å“åŠ›å¤§äºé˜ˆå€¼çš„å±‚
#     selected_layers = [layer for layer, influence in layers_influences if influence > threshold]
#     return selected_layers




# def update_model_with_filtered_layers(model, selected_layers):
#     """
#     æ›´æ–°æ¨¡å‹ç»“æ„ï¼Œä»…ä¿ç•™ç­›é€‰åçš„å±‚ã€‚
#     åŒæ—¶æ‰“å°å‡ºæ›´æ–°å‰åçš„æ¨¡å‹ç»“æ„ï¼Œç¡®ä¿æ›´æ–°æ­£ç¡®ã€‚
#     """
#     print(f"\næ›´æ–°æ¨¡å‹å‰çš„ç»“æ„ï¼š{len(model.layers)}")
#     # for layer in model.layers:
#     #     print(f"  - {layer.name} (mean={layer.mean}, variance={layer.variance})")

#     # æ›´æ–°æ¨¡å‹çš„å±‚
#     model.layers = selected_layers

#     print(f"\næ›´æ–°æ¨¡å‹åçš„ç»“æ„ï¼š{len(model.layers)}")
#     # for layer in model.layers:
#     #     print(f"  - {layer.name} (mean={layer.mean}, variance={layer.variance})")

#     return model
# # # è®¡ç®—å½±å“åŠ›å¹¶ç­›é€‰å±‚
# # layers_influences = calculate_layer_influences(model, alpha=2.0, beta=1.0)
# # selected_layers = filter_layers_by_influence(layers_influences, threshold=0.1)

# # # æ›´æ–°æ¨¡å‹ç»“æ„
# # update_model_with_filtered_layers(model, selected_layers)

# # æ’åºå¹¶ç”Ÿæˆæ–°çš„æ–‡æœ¬æ ¼å¼
# def generate_sorted_text(sorted_influences):
#     sorted_text = "æŒ‰å½±å“åŠ›ä»ä½åˆ°é«˜æ’åºçš„ç»“æœï¼š\n"
#     for name, influence in sorted_influences:
#         sorted_text += f"{name}: å½±å“åŠ› = {influence:.4f}\n"
#     return sorted_text



# # ä¸»å‡½æ•°
# def main(input_file_path, output_file_path, alpha=2.0, beta=1.0):
#     # ä»æ–‡ä»¶è¯»å–æ–‡æœ¬
#     text = read_text_from_file(input_file_path)

#     # è§£ææ–‡æœ¬
#     layers = parse_text(text)

#     if not layers:
#         print("æœªè§£æåˆ°ä»»ä½•å±‚ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶å†…å®¹ï¼")
#         return

#     # è®¡ç®—å½±å“åŠ›å¹¶æ’åº
#     influences = calculate_influence(layers, alpha, beta)
#     sorted_influences = sorted(influences, key=lambda x: x[1])

#     # ç”Ÿæˆæ’åºåçš„æ–‡æœ¬
#     sorted_text = generate_sorted_text(sorted_influences)

#     # å°†ç»“æœå†™å…¥æ–°æ–‡ä»¶
#     with open(output_file_path, "w", encoding="utf-8") as file:
#         file.write(sorted_text)

#     print(f"æ’åºç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{output_file_path}")

# # ç¤ºä¾‹è°ƒç”¨
# if __name__ == "__main__":
#     input_file = "activation_stats.txt"  # è¾“å…¥æ–‡ä»¶è·¯å¾„
#     output_file = "act_output2_1.txt"  # è¾“å‡ºæ–‡ä»¶è·¯å¾„
#     main(input_file, output_file, alpha=2.0, beta=1.0)


# fcdiffusion_reduce_model.py
import re
from collections import namedtuple

import numpy as np

# å®šä¹‰ä¸€ä¸ªæ•°æ®ç»“æ„æ¥å­˜å‚¨æ¯ä¸€å±‚çš„ä¿¡æ¯
LayerStats = namedtuple("LayerStats", ["name", "shape", "mean", "variance"])

# ä»æ–‡ä»¶ä¸­è¯»å–æ–‡æœ¬
def read_text_from_file(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        text = file.read()
    return text

# è§£ææ–‡æœ¬
# def parse_text(text):
#     layers = []
#     # æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œå…è®¸å¤šä½™çš„ç©ºæ ¼å’Œä¸åŒçš„æ‹¬å·
#     pattern = re.compile(r"æ¿€æ´»å€¼ç»Ÿè®¡\s*[\(ï¼ˆ](.*?)[\)ï¼‰]\s*ï¼š\s*å½¢çŠ¶\s*=\s*[\(ï¼ˆ](.*?)[\)ï¼‰]\s*,\s*å‡å€¼\s*=\s*([-+]?\d*\.\d+|\d+)\s*,\s*æ–¹å·®\s*=\s*([-+]?\d*\.\d+|\d+)\s*")
#     matches = pattern.findall(text)

#     if not matches:
#         print("æ­£åˆ™è¡¨è¾¾å¼æœªåŒ¹é…åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼")
#         return layers

#     for match in matches:
#         name, shape, mean, variance = match
#         layers.append(LayerStats(name, shape, float(mean), float(variance)))

#     print(f"layers:{layers}")
#     return layers

import re

# å®šä¹‰ä¸€ä¸ªç±»æ¥å­˜å‚¨å±‚çš„ç»Ÿè®¡ä¿¡æ¯
# class LayerStats:
#     def __init__(self, name, shape, mean, variance):
#         self.name = name
#         self.shape = shape
#         self.mean = mean
#         self.variance = variance

# def parse_text(text):
#     layers = []
#     # è°ƒæ•´æ­£åˆ™è¡¨è¾¾å¼ä»¥æ›´å‡†ç¡®åœ°åŒ¹é…å±‚åç§°å’Œç»Ÿè®¡ä¿¡æ¯
#     pattern = re.compile(
#         r"æ¿€æ´»å€¼ç»Ÿè®¡\s*[\(ï¼ˆ](.*?)[\)ï¼‰]\s*ï¼š\s*å½¢çŠ¶\s*=\s*[\(ï¼ˆ](.*?)[\)ï¼‰]\s*,\s*å‡å€¼\s*=\s*([-+]?\d*\.\d+|\d+)\s*,\s*æ–¹å·®\s*=\s*([-+]?\d*\.\d+|\d+)\s*")
#     matches = pattern.findall(text)

#     if not matches:
#         print("æ­£åˆ™è¡¨è¾¾å¼æœªåŒ¹é…åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼")
#         return layers

#     for match in matches:
#         name, shape, mean, variance = match
#         print(f"name:{name}")
#         # æ¸…ç†å±‚åç§°ä¸­çš„å¤šä½™ç©ºæ ¼
#         name = name.strip()
#         layers.append(LayerStats(name, shape, float(mean), float(variance)))
#     print(f"layers:{layers}")
#     return layers

import re
from collections import namedtuple

# å®šä¹‰ä¸€ä¸ªnamedtupleæ¥å­˜å‚¨å±‚çš„ç»Ÿè®¡ä¿¡æ¯
LayerStats = namedtuple("LayerStats", ["name", "shape", "mean", "variance"])

def parse_text(text):
    layers = []
    print("start parse")
    # ä½¿ç”¨æ›´ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¯ä¸€è¡Œ
    pattern = re.compile(
        r"æ¿€æ´»å€¼ç»Ÿè®¡:(.*),å½¢çŠ¶:(.*),å‡å€¼:(.*),æ–¹å·®:(.*)"
    )
    matches = pattern.finditer(text)
    print(f"matches:{matches}")
    if not matches:
        print("æ­£åˆ™è¡¨è¾¾å¼æœªåŒ¹é…åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼")
        return layers
    
    

    for match in matches:
            name = match.group(1)
            shape = match.group(2)
            mean = match.group(3)
            variance = match.group(4)
            # æ¸…ç†å±‚åç§°ä¸­çš„å¤šä½™ç©ºæ ¼
            # print(f"name:{name}")
            # f.write(name)
            name = name.strip()
            # print(f"name:{name}")
            layers.append(LayerStats(name, shape, float(mean), float(variance)))
    # with open("match_layers.txt",'w') as f:
    #     f.write(layers)
    # print(f"layers:{layers}")
    return layers


# # è®¡ç®—æ¯ä¸€å±‚çš„å½±å“åŠ›
# def calculate_layer_influences(layers, alpha=2.0, beta=1.0):
#     influences = []
#     for layer in layers:
#         influence = alpha * abs(layer.mean) + beta * layer.variance
#         influences.append((layer.name, influence))
#     with open("all_influence_layers.txt", "w",encoding='utf-8') as f:
#         f.write(f"{influences}")
    
#     return influences

def calculate_layer_influences(layers, alpha=1.0, beta=1.0):
    # è®¡ç®—å…¨å±€å‡å€¼å’Œæ–¹å·®
    all_means = [layer.mean for layer in layers]
    all_vars = [layer.variance for layer in layers]
    global_mean = np.mean(all_means)
    global_var = np.mean(all_vars)
    
    # æ ‡å‡†åŒ–å¹¶åŠ æƒ
    # layer_influences = {}
    layer_influences = []
    for layer in layers:
        norm_mean = layer.mean / global_mean
        norm_var = layer.variance / global_var
        influence = alpha * norm_mean + beta * norm_var
        # layer_influences[layer.name] = influence
        layer_influences.append((layer.name, influence))
    with open("all_influence_layers.txt", "w",encoding='utf-8') as f:
        f.write(f"{layer_influences}")
    return layer_influences



def filter_layers(influences, method='topk', k=0.5, n_std=1.0):
    scores = list(influences.values())
    if method == 'topk':
        threshold = np.percentile(scores, (1 - k) * 100)
    elif method == 'std':
        mean = np.mean(scores)
        std = np.std(scores)
        threshold = mean + n_std * std
    return {name: score for name, score in influences.items() if score >= threshold}
# # ç­›é€‰å½±å“åŠ›å¤§äºé˜ˆå€¼çš„å±‚
# def filter_layers_by_influence(layers_influences, threshold=0.1):
#     selected_layers = [layer_name for layer_name, influence in layers_influences if influence > threshold 
#                        and "diffusion_model" in layer_name and ('Conv2D' or 'Resblock') in layer_name ]
#     with open("selected_layers.txt", "w",encoding='utf-8') as f:
#         f.write(f"{selected_layers}")
#     return selected_layers


# # è®¡ç®—æ¯ä¸€å±‚çš„å½±å“åŠ› 
# def calculate_layer_influences(layers, alpha=2.0, beta=1.0):
#     """
#     layers: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åº”åŒ…å«å±æ€§ï¼š
#        - name: å±‚çš„åç§°ï¼ˆå¯ä»¥åŒ…å«å‰ç¼€ï¼‰
#        - mean: æ¿€æ´»å€¼å‡å€¼
#        - variance: æ¿€æ´»å€¼æ–¹å·®
#     """
#     influences = []
#     for layer in layers:
#         if hasattr(layer, "mean") and hasattr(layer, "variance"):
#             influence = alpha * abs(layer.mean) + beta * layer.variance
#             influences.append((layer.name, influence))
#     influences.sort(key=lambda x: x[1], reverse=True)
#     with open("all_influence_layers.txt", "w", encoding="utf-8") as f:
#         f.write(f"{influences}\n")
#     return influences

# ç­›é€‰å‡ºå‰ªææ—¶éœ€è¦ä¿ç•™å’Œå‰ªæçš„å±‚
def filter_layers_by_influence(layers_influences, threshold=1):
    # candidate_keywords = ["conv2d", "resblock", "linear"]
    candidate_keywords = ["conv2d", "resblock"]
    # ä»…ä¿ç•™å±äº diffusion_model çš„å±‚ï¼ˆè¿‡æ»¤æ¡ä»¶ï¼šåç§°ä¸­å«æœ‰ 'diffusion_model'ï¼‰
    diffusion_layers = [
        (name, influence) for name, influence in layers_influences 
        if "diffusion_model" in name.lower()
    ]
    # print(f"diffusion_layers:{diffusion_layers}")
    
    # åœ¨ diffusion_model å†…çš„å±‚ä¸­ï¼Œå€™é€‰å±‚ï¼šåç§°ä¸­åŒ…å«å€™é€‰å…³é”®å­—
    candidate_layers = [
        layer_name for layer_name, _ in diffusion_layers 
        if any(keyword in layer_name.lower() for keyword in candidate_keywords)
    ]
    
    # print(f"candidate_layers:{candidate_layers}")
    # ä¿ç•™å±‚ï¼šå€™é€‰å±‚ä¸­å½±å“åŠ›å¤§äºé˜ˆå€¼çš„å±‚
    keep_layers = [
        layer_name for layer_name, influence in diffusion_layers 
        if abs(influence) > threshold and any(keyword in layer_name.lower() for keyword in candidate_keywords)
    ]
    
    # å‰ªæå±‚ï¼šå€™é€‰å±‚ä¸­ä¸åœ¨ä¿ç•™å±‚å†…çš„éƒ¨åˆ†
    prune_layers = [layer for layer in candidate_layers if layer not in keep_layers]
    
    with open("keep_layers.txt", "w", encoding="utf-8") as f:
        f.write(f"{keep_layers}")
    with open("prune_layers.txt", "w", encoding="utf-8") as f:
        f.write(f"{prune_layers}")
    
    return keep_layers, prune_layers


# æ›´æ–°æ¨¡å‹ç»“æ„ï¼Œä»…ä¿ç•™ç­›é€‰åçš„å±‚
def update_model_with_filtered_layers(model, selected_layers):
    """
    æ›´æ–°æ¨¡å‹ç»“æ„ï¼Œä»…ä¿ç•™ç­›é€‰åçš„å±‚ã€‚
    åŒæ—¶æ‰“å°å‡ºæ›´æ–°å‰åçš„æ¨¡å‹ç»“æ„ï¼Œç¡®ä¿æ›´æ–°æ­£ç¡®ã€‚
    """
    print(f"\næ›´æ–°æ¨¡å‹å‰çš„ç»“æ„ï¼š{len(model.layers)}å±‚")
    original_layers = model.layers
    model.layers = [layer for layer in original_layers if layer.name in selected_layers]

    print(f"\næ›´æ–°æ¨¡å‹åçš„ç»“æ„ï¼š{len(model.layers)}å±‚")
    return model

# æ’åºå¹¶ç”Ÿæˆæ–°çš„æ–‡æœ¬æ ¼å¼
def generate_sorted_text(sorted_influences):
    sorted_text = "æŒ‰å½±å“åŠ›ä»ä½åˆ°é«˜æ’åºçš„ç»“æœï¼š\n"
    for name, influence in sorted_influences:
        sorted_text += f"{name}: å½±å“åŠ› = {influence:.4f}\n"
    return sorted_text

# ä¸»å‡½æ•°
def main(input_file_path, model):
    """
    ä¸»å‡½æ•°ï¼Œä»æ–‡æœ¬æ–‡ä»¶ä¸­è¯»å–å±‚æ¬¡ä¿¡æ¯ä¸€æ¬¡ï¼Œè®¡ç®—å½±å“åŠ›ï¼Œç­›é€‰å±‚æ¬¡ï¼Œå¹¶æ›´æ–°æ¨¡å‹ç»“æ„ã€‚
    """
    # ä»æ–‡ä»¶è¯»å–æ–‡æœ¬
    text = read_text_from_file(input_file_path)

    # è§£ææ–‡æœ¬
    layers = parse_text(text)

    if not layers:
        print("æœªè§£æåˆ°ä»»ä½•å±‚ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶å†…å®¹ï¼")
        return

    # è®¡ç®—æ¯ä¸€å±‚çš„å½±å“åŠ›
    layers_influences = calculate_layer_influences(layers, alpha=1.0, beta=1.0)
    sorted_influences = sorted(layers_influences, key=lambda x: x[1])

    # ç­›é€‰å½±å“åŠ›å¤§äºé˜ˆå€¼çš„å±‚
    # selected_layers = filter_layers_by_influence(layers_influences, threshold=0.1)

    # # æ›´æ–°æ¨¡å‹ç»“æ„
    # updated_model = update_model_with_filtered_layers(model, selected_layers)

    # # ç”Ÿæˆæ’åºåçš„æ–‡æœ¬
    # sorted_text = generate_sorted_text(sorted_influences)

    # # æ‰“å°æ’åºç»“æœ
    # print(sorted_text)

    # return updated_model


# # å®šä¹‰å­¦ç”Ÿæ¨¡å‹ç±»
# class StudentModel(nn.Module):
#     def __init__(self, original_model, layers_to_keep):
#         super(StudentModel, self).__init__()
#         self.layers = nn.ModuleList()
#         self.layer_names = []  # ç”¨äºè®°å½•ä¿ç•™çš„å±‚åç§°
#         self.original_model = original_model  # ä¿ç•™åŸå§‹æ¨¡å‹çš„å¼•ç”¨

#         # éå†åŸå§‹æ¨¡å‹çš„æ¨¡å—ï¼Œæå–éœ€è¦ä¿ç•™çš„å±‚
#         for name, module in original_model.named_modules():
#             if name in layers_to_keep:
#                 self.layers.append(module)
#                 self.layer_names.append(name)
#         print(f"ä¿ç•™çš„å±‚: {self.layer_names}")

#     def forward(self, x):
#         for layer in self.layers:
#             x = layer(x)
#         return x

#     def load_weights_from_original_model(self):
#         """ä»åŸå§‹æ¨¡å‹ä¸­åŠ è½½æƒé‡"""
#         for student_layer, original_layer_name in zip(self.layers, self.layer_names):
#             original_layer = self.original_model.get_submodule(original_layer_name)
#             student_layer.load_state_dict(original_layer.state_dict())


def create_pruned_diffusion_model(original_diffusion_model, layers_to_keep):
    class PrunedDiffusionModel(nn.Module):
        def __init__(self, parent_module, parent_path=""):
            super().__init__()
            self.layers = nn.ModuleDict()
            self._build_pruned_structure(parent_module, parent_path)

        def _build_pruned_structure(self, module, current_path):
            """é€’å½’æ„å»ºä¿®å‰ªåçš„æ¨¡å‹ç»“æ„"""
            for name, child_module in module.named_children():
                # æ„å»ºå®Œæ•´è·¯å¾„ï¼ˆå…¼å®¹ä¸¤ç§æ ¼å¼ï¼‰
                full_path = f"{current_path}.{name}" if current_path else name
                formatted_path_v1 = f"model.diffusion_model.{full_path}_{type(child_module).__name__}"
                # formatted_path_v2 = f"{full_path}_{type(child_module).__name__}"

                # æ£€æŸ¥æ˜¯å¦åœ¨ä¿ç•™åˆ—è¡¨ä¸­
                if any(layer in formatted_path_v1 for layer in layers_to_keep):
                    self.layers[full_path] = child_module
                    print(f"âœ… ä¿ç•™å±‚: {full_path} (åŒ¹é…åˆ° {formatted_path_v1} )")
                else:
                    # é€’å½’å¤„ç†å­æ¨¡å—
                    sub_module = PrunedDiffusionModel(child_module, full_path)
                    if len(sub_module.layers) > 0:
                        self.layers[full_path] = sub_module
                        print(f"ğŸ” ä¿ç•™å­æ ‘: {full_path}")

        def forward(self, x, c=None):
            for name, layer in self.layers.items():
                if isinstance(layer, PrunedDiffusionModel):
                    x = layer(x, c)
                else:
                    x = layer(x)
            return x

    # åˆå§‹åŒ–å¹¶æ‰“å°æœ€ç»ˆç»“æ„
    pruned_model = PrunedDiffusionModel(original_diffusion_model)
    print("\næœ€ç»ˆå‰ªæç»“æ„ï¼š")
    for name, module in pruned_model.layers.items():
        print(f"â””â”€ {name} ({type(module).__name__})")
    return pruned_model

# ç¤ºä¾‹è°ƒç”¨
if __name__ == "__main__":
        # ç­›é€‰é‡è¦å±‚
    influence_threshold = 0.1  # è®¾ç½®å½±å“åŠ›å€¼çš„é˜ˆå€¼
    input_file = "activation_stats.txt"  # è¾“å…¥æ–‡ä»¶è·¯å¾„
    
    text = read_text_from_file(input_file)
    
    # è§£ææ–‡æœ¬
    layers = parse_text(text)
    layers_influences = calculate_layer_influences(layers, alpha=1.0, beta=1.0)
    
    layers_to_keep = filter_layers_by_influence(layers_influences, threshold=1.3)
    
    
    # # æ„å»ºå­¦ç”Ÿæ¨¡å‹
    # student_model = StudentModel(model, layers_to_keep).to(device)
    
    # # ä»åŸå§‹æ¨¡å‹ä¸­åŠ è½½æƒé‡
    # student_model.load_weights_from_original_model()
    # # ä¿å­˜å­¦ç”Ÿæ¨¡å‹çš„æƒé‡
    # student_ckpt_path = "student_model_checkpoint.ckpt"
    # torch.save(student_model.state_dict(), student_ckpt_path)
    # print(f"å­¦ç”Ÿæ¨¡å‹å·²ä¿å­˜åˆ° {student_ckpt_path}")
    